{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Model Forge","text":"<p>A framework to easily create machine learning models. It provides part for the orchestration and datapipeline of the machine learning model. </p>"},{"location":"#contribution","title":"Contribution","text":"<p>To make contributions to this project first clone the repo: <pre><code>git clone https://github.com/BartvanWoesik/model-forge.git\npip install poetry\npoetry install \n</code></pre></p> <p>To setup up pre-commit:</p> <pre><code>pre-commit install\n</code></pre>"},{"location":"datapipeline/","title":"Data Pipeline","text":""},{"location":"datapipeline/#model_forge.data.datapipeline.PandasDataPipeline","title":"<code>PandasDataPipeline</code>","text":"<p>A data pipeline class that applies a series of steps to a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>list</code> <p>A list of functions or tuples (description, function) representing the steps to be applied.</p> required <code>name</code> <code>str</code> <p>The name of the pipeline. Defaults to \"pipeline\".</p> <code>'pipeline'</code> <p>Attributes:</p> Name Type Description <code>steps</code> <code>list</code> <p>A list of functions or tuples (description, function) representing the steps to be applied.</p> <code>name</code> <code>str</code> <p>The name of the pipeline.</p> <p>Methods:</p> Name Description <code>apply</code> <p>pd.DataFrame) -&gt; pd.DataFrame: Applies the pipeline steps to the given DataFrame.</p> Source code in <code>model_forge/data/datapipeline.py</code> <pre><code>class PandasDataPipeline:\n    \"\"\"\n    A data pipeline class that applies a series of steps to a pandas DataFrame.\n\n    Args:\n        steps (list): A list of functions or tuples (description, function) representing the steps to be applied.\n        name (str, optional): The name of the pipeline. Defaults to \"pipeline\".\n\n    Attributes:\n        steps (list): A list of functions or tuples (description, function) representing the steps to be applied.\n        name (str): The name of the pipeline.\n\n    Methods:\n        apply(df: pd.DataFrame) -&gt; pd.DataFrame:\n            Applies the pipeline steps to the given DataFrame.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        steps,\n        name: str = \"pipeline\",\n    ) -&gt; None:\n        self.steps = steps\n        self.name = name\n\n    def _apply(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Applies the pipeline steps to the given DataFrame.\n\n        Args:\n            df (pd.DataFrame): The DataFrame to apply the steps to.\n\n        Returns:\n            pd.DataFrame: The DataFrame after applying the steps.\n\n        Raises:\n            TypeError: If a step function does not accept a pandas DataFrame as an argument.\n\n        \"\"\"\n        for step_number, step in enumerate(self.steps, start=0):\n            if isinstance(step, tuple):\n                # If step is a tuple, assume it's (description, function)\n                _, step_func = step\n            else:\n                step_func = step\n\n            # Check if step_func expects a pandas DataFrame as its argument\n            if not self._function_accepts_dataframe(step_func):\n                raise TypeError(\n                    f\"The step function at step {step_number} does not accept a pandas DataFrame as an argument.\"\n                )\n            # Apply the step\n            df = step_func(df)\n\n        return df\n\n    def _function_accepts_dataframe(self, func):\n        \"\"\"Check if first argument op function expects pd.DataFrame\"\"\"\n        sig = inspect.signature(func)\n        params = sig.parameters.values()\n        first_param = next(iter(params), None)\n        return first_param and first_param.annotation is pd.DataFrame\n\n    @safe\n    def apply(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Applies the pipeline steps to the given DataFrame.\n\n        Args:\n            df (pd.DataFrame): The DataFrame to apply the steps to.\n\n        Returns:\n            pd.DataFrame: The DataFrame after applying the steps.\n\n        \"\"\"\n        return self._apply(df)\n</code></pre>"},{"location":"datapipeline/#model_forge.data.datapipeline.PandasDataPipeline.apply","title":"<code>apply(df)</code>","text":"<p>Applies the pipeline steps to the given DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to apply the steps to.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The DataFrame after applying the steps.</p> Source code in <code>model_forge/data/datapipeline.py</code> <pre><code>@safe\ndef apply(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Applies the pipeline steps to the given DataFrame.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to apply the steps to.\n\n    Returns:\n        pd.DataFrame: The DataFrame after applying the steps.\n\n    \"\"\"\n    return self._apply(df)\n</code></pre>"},{"location":"datapipeline/#model_forge.data.datapipeline.safe","title":"<code>safe(fn)</code>","text":"<p>A decorator that creates a safe version of the decorated function. The safe version of the function makes a deep copy of the arguments and keyword arguments before calling the original function. This ensures that the original arguments are not modified during the function call.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>function</code> <p>The function to be decorated.</p> required <p>Returns:</p> Name Type Description <code>function</code> <p>The safe version of the decorated function.</p> Source code in <code>model_forge/data/datapipeline.py</code> <pre><code>def safe(fn):\n    \"\"\"\n    A decorator that creates a safe version of the decorated function.\n    The safe version of the function makes a deep copy of the arguments\n    and keyword arguments before calling the original function.\n    This ensures that the original arguments are not modified during the function call.\n\n    Args:\n        fn (function): The function to be decorated.\n\n    Returns:\n        function: The safe version of the decorated function.\n    \"\"\"\n\n    @wraps(fn)\n    def wrapper(self, *args, **kwargs):\n        cp_args = deepcopy(args)\n        cp_kwargs = deepcopy(kwargs)\n        res = fn(self, *cp_args, **cp_kwargs)\n        return res\n\n    return wrapper\n</code></pre>"},{"location":"dataset/","title":"Dataset","text":""},{"location":"dataset/#model_forge.data.dataset.Dataset","title":"<code>Dataset</code>","text":"<p>             Bases: <code>dict</code></p> <p>A class representing a dataset.</p> <p>This class extends the built-in <code>dict</code> class and provides additional functionality for working with datasets.</p> <p>Attributes:</p> Name Type Description <code>data_splitter</code> <p>An optional data splitter object used to split the data into train and test sets.</p> <code>target_column</code> <p>The name of the target column in the data.</p> <code>name</code> <p>The name of the dataset.</p> <code>_is_data_splitted</code> <p>A flag indicating whether the data has been split.</p> <code>data</code> <p>The input data for the dataset.</p> <code>_X</code> <p>The feature matrix X.</p> <code>_y</code> <p>The target variable array.</p> <code>splits</code> <p>A dictionary containing the splits of the dataset.</p> <p>Methods:</p> Name Description <code>X</code> <p>Returns the feature matrix X.</p> <code>y</code> <p>Returns the target variable array.</p> <code>columns</code> <p>Returns the list of column names.</p> <code>shape</code> <p>Returns the shape of the feature matrix X.</p> <code>_split_data</code> <p>Splits the data into train and test sets.</p> <code>_run_checks</code> <p>Runs checks on the splits to ensure data integrity.</p> <code>load_split</code> <p>Loads a specific split of the dataset.</p> <code>load_train_test</code> <p>Loads the training and testing data splits from the dataset.</p> <code>create_from_pipeline</code> <p>Creates a dataset from a data loading function and optional data pipeline.</p> <code>create_from_splits</code> <p>Creates a dataset from splits.</p> Source code in <code>model_forge/data/dataset.py</code> <pre><code>class Dataset(dict):\n    \"\"\"\n    A class representing a dataset.\n\n    This class extends the built-in `dict` class and provides additional functionality for working with datasets.\n\n    Attributes:\n        data_splitter: An optional data splitter object used to split the data into train and test sets.\n        target_column: The name of the target column in the data.\n        name: The name of the dataset.\n        _is_data_splitted: A flag indicating whether the data has been split.\n        data: The input data for the dataset.\n        _X: The feature matrix X.\n        _y: The target variable array.\n        splits: A dictionary containing the splits of the dataset.\n\n    Methods:\n        X: Returns the feature matrix X.\n        y: Returns the target variable array.\n        columns: Returns the list of column names.\n        shape: Returns the shape of the feature matrix X.\n        _split_data: Splits the data into train and test sets.\n        _run_checks: Runs checks on the splits to ensure data integrity.\n        load_split: Loads a specific split of the dataset.\n        load_train_test: Loads the training and testing data splits from the dataset.\n        create_from_pipeline: Creates a dataset from a data loading function and optional data pipeline.\n        create_from_splits: Creates a dataset from splits.\n    \"\"\"\n\n    def __init__(\n        self,\n        data: pd.DataFrame,\n        data_splitter=None,\n        target_column: str = \"y\",\n        name: str = \"dataset\",\n        splits_columns: list = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize a Dataset object.\n\n        Args:\n            data (pd.DataFrame): The input data for the dataset.\n            data_splitter (optional): An optional data splitter object used to split the data into train and test sets.\n            target_column (str): The name of the target column in the data.\n            name (str): The name of the dataset.\n\n        Returns:\n            None\n        \"\"\"\n\n        self.data_splitter = data_splitter\n        self.target_column = target_column\n        self.splits_columns = splits_columns\n        self.name = name\n        self._is_data_splitted = False\n        self.data = data\n\n        self._split_data()\n        super().__init__(self.splits)\n\n    @property\n    def X(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Returns the feature matrix X.\n\n        Returns:\n            pd.DataFrame: The feature matrix X.\n        \"\"\"\n        return self[\"ALL\"][0]\n\n    @property\n    def y(self) -&gt; np.array:\n        \"\"\"\n        Returns the target variable array.\n\n        Returns:\n            np.array: The target variable array.\n        \"\"\"\n        return self[\"ALL\"][1]\n\n    @property\n    def columns(self):\n        \"\"\"\n        Returns a list of column names in the dataset.\n\n        Returns:\n            list: A list of column names.\n        \"\"\"\n        return list(self.splits.values())[0][0].columns.tolist()\n\n    @property\n    def shape(self):\n        \"\"\"\n        Returns the shape of the dataset.\n\n        Returns:\n            tuple: A tuple representing the shape of the dataset.\n        \"\"\"\n        return self.X.shape\n\n    def _split_data(self) -&gt; None:\n        \"\"\"\n        Split the data into train and test sets.\n\n        This method splits the data into train and test sets based on the provided data splitter.\n        If no data splitter is provided, it assumes all the data is the train set.\n\n        Returns:\n            None\n        \"\"\"\n        self.splits = {}\n        self.splits[\"ALL\"] = [True] * len(self.data)\n        if self.splits_columns is not None:\n            for column in self.splits_columns:\n                self.splits[column] = list(self.data[column] == 1)\n\n        self._is_data_splitted = True\n        self._run_checks()\n\n    def __getitem__(self, key: Any) -&gt; Any:\n        \"\"\"\n        Retrieve an item from the dataset.\n\n        Args:\n            key (Any): The key used to retrieve the item.\n\n        Returns:\n            Any: The item corresponding to the given key.\n        \"\"\"\n        indexes = super().__getitem__(key)\n        return (\n            self.data.drop(columns=self.target_column)[indexes],\n            self.data[self.target_column][indexes],\n        )\n\n    def _run_checks(self) -&gt; None:\n        \"\"\"\n        Run checks on the splits of the dataset.\n\n        Raises:\n            AssertionError: If any of the splits is None, not a list, or empty.\n        \"\"\"\n        for split_name, indexes in self.splits.items():\n            assert indexes is not None, f\"Split '{split_name}' is None\"\n            assert isinstance(indexes, list), f\"Split '{split_name}' is not a list\"\n            assert len(indexes) != 0, f\"Split '{split_name}' is empty\"\n\n    def __getattr__(self, attr_name: str) -&gt; Any:\n        \"\"\"\n        Retrieves the attribute specified by __name.\n\n        Args:\n            __name (str): The name of the attribute to retrieve.\n\n        Returns:\n            Any: The value of the attribute.\n\n        Raises:\n            AttributeError: If the attribute specified by __name is not found.\n        \"\"\"\n\n        if attr_name.startswith((\"X_\", \"y_\")):\n            try:\n                _, split_name = attr_name.split(\"_\", 1)\n                if split_name in self.keys():\n                    return (\n                        self[split_name][0]\n                        if attr_name.startswith(\"X_\")\n                        else self[split_name][1]\n                    )\n            except AttributeError as e:\n                raise AttributeError(\n                    f\"Split '{attr_name}' not found. Attribute. Original error: {str(e)}\"\n                )\n\n        if not attr_name.startswith((\"X_\", \"y_\")):\n            try:\n                return super().__getattr__(attr_name)\n            except AttributeError as e:\n                raise AttributeError(\n                    f\"Attribute '{attr_name}' not found. Original error: {str(e)}\"\n                )\n\n        raise AttributeError(f\"Attribute '{attr_name}' not found\")\n\n    def __iter__(self):\n        self._iter_keys = iter(self.keys())\n        return self\n\n    def __next__(self):\n        key = next(self._iter_keys)\n        return key, (self[key][0], self[key][1])\n\n    def load_split(\n        self,\n        split: str,\n        return_X_y: bool = False,\n        sample_n_rows: Optional[int] = None,\n        random_state: int = 36,\n    ) -&gt; Union[tuple[pd.DataFrame, np.array], pd.DataFrame]:\n        \"\"\"\n        Load a specific split of the dataset.\n\n        Args:\n            split (str): The name of the split to load.\n            return_X_y (bool, optional): Whether to return X and y separately. Defaults to False.\n            sample_n_rows (int, optional): Number of rows to sample from the split. Defaults to None.\n            random_state (int, optional): Random state for sampling rows. Defaults to 36.\n\n        Returns:\n            Union[tuple[pd.DataFrame, np.array], pd.DataFrame]: The loaded split of the dataset.\n                If return_X_y is True, returns a tuple of X and y.\n                If return_X_y is False, returns a DataFrame with X and y as columns.\n        \"\"\"\n\n        if not self._is_data_splitted:\n            self._split_data()\n        if split not in self.splits.keys():\n            raise ValueError(\n                f\"Invalid Split: You requested split '{split}'. Valid splits are: {*list(self.splits.keys()),} \"\n            )\n        X, y = self[split][0], self[split][1]\n        if sample_n_rows is not None:\n            X = X.sample(sample_n_rows, random_state=random_state)\n            y = y[X.index]\n\n        if return_X_y:\n            return X, y\n        else:\n            return X.assign(**{self.target_column: y})\n\n    @classmethod\n    def create_from_pipeline(\n        cls,\n        data_loading_function: Callable[[], pd.DataFrame],\n        data_pipeline=None,\n        data_splitter=None,\n        target_column=\"y\",\n        name: str = \"dataset\",\n        splits_columns=None,\n    ):\n        \"\"\"\n        Create a dataset from a data loading function and optional data pipeline.\n\n        Args:\n            cls: The class of the dataset.\n            data_loading_function: A function that loads the data and returns a pandas DataFrame.\n            data_pipeline: An optional data pipeline to apply to the loaded data.\n            data_splitter: An optional data splitter to split the data into train and test sets.\n            target_column: The name of the target column in the dataset.\n            name: The name of the dataset.\n\n        Returns:\n            An instance of the dataset class.\n\n        \"\"\"\n        data = data_loading_function()\n        if data_pipeline:\n            data = data_pipeline.apply(data)\n        return cls(\n            data=data,\n            data_splitter=data_splitter,\n            target_column=target_column,\n            name=name,\n            splits_columns=splits_columns,\n        )\n\n    @classmethod\n    def create_from_splits(\n        cls,\n        splits: dict[str, tuple[pd.DataFrame, np.array]],\n        name: str = \"dataset\",\n        target_column: str = \"y\",\n    ):\n        \"\"\"\n        Create a dataset from splits.\n\n        Args:\n            cls (class): The class of the dataset.\n            splits (dict[str, tuple[pd.DataFrame, np.array]]): A dictionary containing the splits of the dataset.\n                Each split is represented as a tuple of a pandas DataFrame (X) and a numpy array (y).\n            name (str, optional): The name of the dataset. Defaults to \"dataset\".\n            target_column (str, optional): The name of the target column. Defaults to \"y\".\n\n        Returns:\n            dataset (cls): The created dataset.\n        \"\"\"\n        Xs = []\n        for split_name, (X, y) in splits.items():\n            assert (\n                target_column not in X.columns\n            ), f\"Split {split_name} already has a target column ({target_column}), please drop or rename\"\n            Xs.append(X.assign(y=y))\n        fullX = pd.concat(Xs, ignore_index=True)\n\n        dataset = cls(\n            data=fullX, data_splitter=None, target_column=target_column, name=name\n        )\n        dataset._is_data_splitted = True\n        dataset.splits = splits\n        dataset._run_checks()\n        return dataset\n</code></pre>"},{"location":"dataset/#model_forge.data.dataset.Dataset.X","title":"<code>X: pd.DataFrame</code>  <code>property</code>","text":"<p>Returns the feature matrix X.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The feature matrix X.</p>"},{"location":"dataset/#model_forge.data.dataset.Dataset.columns","title":"<code>columns</code>  <code>property</code>","text":"<p>Returns a list of column names in the dataset.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of column names.</p>"},{"location":"dataset/#model_forge.data.dataset.Dataset.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>Returns the shape of the dataset.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple representing the shape of the dataset.</p>"},{"location":"dataset/#model_forge.data.dataset.Dataset.y","title":"<code>y: np.array</code>  <code>property</code>","text":"<p>Returns the target variable array.</p> <p>Returns:</p> Type Description <code>array</code> <p>np.array: The target variable array.</p>"},{"location":"dataset/#model_forge.data.dataset.Dataset.__getattr__","title":"<code>__getattr__(attr_name)</code>","text":"<p>Retrieves the attribute specified by __name.</p> <p>Parameters:</p> Name Type Description Default <code>__name</code> <code>str</code> <p>The name of the attribute to retrieve.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The value of the attribute.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the attribute specified by __name is not found.</p> Source code in <code>model_forge/data/dataset.py</code> <pre><code>def __getattr__(self, attr_name: str) -&gt; Any:\n    \"\"\"\n    Retrieves the attribute specified by __name.\n\n    Args:\n        __name (str): The name of the attribute to retrieve.\n\n    Returns:\n        Any: The value of the attribute.\n\n    Raises:\n        AttributeError: If the attribute specified by __name is not found.\n    \"\"\"\n\n    if attr_name.startswith((\"X_\", \"y_\")):\n        try:\n            _, split_name = attr_name.split(\"_\", 1)\n            if split_name in self.keys():\n                return (\n                    self[split_name][0]\n                    if attr_name.startswith(\"X_\")\n                    else self[split_name][1]\n                )\n        except AttributeError as e:\n            raise AttributeError(\n                f\"Split '{attr_name}' not found. Attribute. Original error: {str(e)}\"\n            )\n\n    if not attr_name.startswith((\"X_\", \"y_\")):\n        try:\n            return super().__getattr__(attr_name)\n        except AttributeError as e:\n            raise AttributeError(\n                f\"Attribute '{attr_name}' not found. Original error: {str(e)}\"\n            )\n\n    raise AttributeError(f\"Attribute '{attr_name}' not found\")\n</code></pre>"},{"location":"dataset/#model_forge.data.dataset.Dataset.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Retrieve an item from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Any</code> <p>The key used to retrieve the item.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The item corresponding to the given key.</p> Source code in <code>model_forge/data/dataset.py</code> <pre><code>def __getitem__(self, key: Any) -&gt; Any:\n    \"\"\"\n    Retrieve an item from the dataset.\n\n    Args:\n        key (Any): The key used to retrieve the item.\n\n    Returns:\n        Any: The item corresponding to the given key.\n    \"\"\"\n    indexes = super().__getitem__(key)\n    return (\n        self.data.drop(columns=self.target_column)[indexes],\n        self.data[self.target_column][indexes],\n    )\n</code></pre>"},{"location":"dataset/#model_forge.data.dataset.Dataset.__init__","title":"<code>__init__(data, data_splitter=None, target_column='y', name='dataset', splits_columns=None)</code>","text":"<p>Initialize a Dataset object.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input data for the dataset.</p> required <code>data_splitter</code> <code>optional</code> <p>An optional data splitter object used to split the data into train and test sets.</p> <code>None</code> <code>target_column</code> <code>str</code> <p>The name of the target column in the data.</p> <code>'y'</code> <code>name</code> <code>str</code> <p>The name of the dataset.</p> <code>'dataset'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>model_forge/data/dataset.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame,\n    data_splitter=None,\n    target_column: str = \"y\",\n    name: str = \"dataset\",\n    splits_columns: list = None,\n) -&gt; None:\n    \"\"\"\n    Initialize a Dataset object.\n\n    Args:\n        data (pd.DataFrame): The input data for the dataset.\n        data_splitter (optional): An optional data splitter object used to split the data into train and test sets.\n        target_column (str): The name of the target column in the data.\n        name (str): The name of the dataset.\n\n    Returns:\n        None\n    \"\"\"\n\n    self.data_splitter = data_splitter\n    self.target_column = target_column\n    self.splits_columns = splits_columns\n    self.name = name\n    self._is_data_splitted = False\n    self.data = data\n\n    self._split_data()\n    super().__init__(self.splits)\n</code></pre>"},{"location":"dataset/#model_forge.data.dataset.Dataset.create_from_pipeline","title":"<code>create_from_pipeline(data_loading_function, data_pipeline=None, data_splitter=None, target_column='y', name='dataset', splits_columns=None)</code>  <code>classmethod</code>","text":"<p>Create a dataset from a data loading function and optional data pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <p>The class of the dataset.</p> required <code>data_loading_function</code> <code>Callable[[], DataFrame]</code> <p>A function that loads the data and returns a pandas DataFrame.</p> required <code>data_pipeline</code> <p>An optional data pipeline to apply to the loaded data.</p> <code>None</code> <code>data_splitter</code> <p>An optional data splitter to split the data into train and test sets.</p> <code>None</code> <code>target_column</code> <p>The name of the target column in the dataset.</p> <code>'y'</code> <code>name</code> <code>str</code> <p>The name of the dataset.</p> <code>'dataset'</code> <p>Returns:</p> Type Description <p>An instance of the dataset class.</p> Source code in <code>model_forge/data/dataset.py</code> <pre><code>@classmethod\ndef create_from_pipeline(\n    cls,\n    data_loading_function: Callable[[], pd.DataFrame],\n    data_pipeline=None,\n    data_splitter=None,\n    target_column=\"y\",\n    name: str = \"dataset\",\n    splits_columns=None,\n):\n    \"\"\"\n    Create a dataset from a data loading function and optional data pipeline.\n\n    Args:\n        cls: The class of the dataset.\n        data_loading_function: A function that loads the data and returns a pandas DataFrame.\n        data_pipeline: An optional data pipeline to apply to the loaded data.\n        data_splitter: An optional data splitter to split the data into train and test sets.\n        target_column: The name of the target column in the dataset.\n        name: The name of the dataset.\n\n    Returns:\n        An instance of the dataset class.\n\n    \"\"\"\n    data = data_loading_function()\n    if data_pipeline:\n        data = data_pipeline.apply(data)\n    return cls(\n        data=data,\n        data_splitter=data_splitter,\n        target_column=target_column,\n        name=name,\n        splits_columns=splits_columns,\n    )\n</code></pre>"},{"location":"dataset/#model_forge.data.dataset.Dataset.create_from_splits","title":"<code>create_from_splits(splits, name='dataset', target_column='y')</code>  <code>classmethod</code>","text":"<p>Create a dataset from splits.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>class</code> <p>The class of the dataset.</p> required <code>splits</code> <code>dict[str, tuple[DataFrame, array]]</code> <p>A dictionary containing the splits of the dataset. Each split is represented as a tuple of a pandas DataFrame (X) and a numpy array (y).</p> required <code>name</code> <code>str</code> <p>The name of the dataset. Defaults to \"dataset\".</p> <code>'dataset'</code> <code>target_column</code> <code>str</code> <p>The name of the target column. Defaults to \"y\".</p> <code>'y'</code> <p>Returns:</p> Name Type Description <code>dataset</code> <code>cls</code> <p>The created dataset.</p> Source code in <code>model_forge/data/dataset.py</code> <pre><code>@classmethod\ndef create_from_splits(\n    cls,\n    splits: dict[str, tuple[pd.DataFrame, np.array]],\n    name: str = \"dataset\",\n    target_column: str = \"y\",\n):\n    \"\"\"\n    Create a dataset from splits.\n\n    Args:\n        cls (class): The class of the dataset.\n        splits (dict[str, tuple[pd.DataFrame, np.array]]): A dictionary containing the splits of the dataset.\n            Each split is represented as a tuple of a pandas DataFrame (X) and a numpy array (y).\n        name (str, optional): The name of the dataset. Defaults to \"dataset\".\n        target_column (str, optional): The name of the target column. Defaults to \"y\".\n\n    Returns:\n        dataset (cls): The created dataset.\n    \"\"\"\n    Xs = []\n    for split_name, (X, y) in splits.items():\n        assert (\n            target_column not in X.columns\n        ), f\"Split {split_name} already has a target column ({target_column}), please drop or rename\"\n        Xs.append(X.assign(y=y))\n    fullX = pd.concat(Xs, ignore_index=True)\n\n    dataset = cls(\n        data=fullX, data_splitter=None, target_column=target_column, name=name\n    )\n    dataset._is_data_splitted = True\n    dataset.splits = splits\n    dataset._run_checks()\n    return dataset\n</code></pre>"},{"location":"dataset/#model_forge.data.dataset.Dataset.load_split","title":"<code>load_split(split, return_X_y=False, sample_n_rows=None, random_state=36)</code>","text":"<p>Load a specific split of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>str</code> <p>The name of the split to load.</p> required <code>return_X_y</code> <code>bool</code> <p>Whether to return X and y separately. Defaults to False.</p> <code>False</code> <code>sample_n_rows</code> <code>int</code> <p>Number of rows to sample from the split. Defaults to None.</p> <code>None</code> <code>random_state</code> <code>int</code> <p>Random state for sampling rows. Defaults to 36.</p> <code>36</code> <p>Returns:</p> Type Description <code>Union[tuple[DataFrame, array], DataFrame]</code> <p>Union[tuple[pd.DataFrame, np.array], pd.DataFrame]: The loaded split of the dataset. If return_X_y is True, returns a tuple of X and y. If return_X_y is False, returns a DataFrame with X and y as columns.</p> Source code in <code>model_forge/data/dataset.py</code> <pre><code>def load_split(\n    self,\n    split: str,\n    return_X_y: bool = False,\n    sample_n_rows: Optional[int] = None,\n    random_state: int = 36,\n) -&gt; Union[tuple[pd.DataFrame, np.array], pd.DataFrame]:\n    \"\"\"\n    Load a specific split of the dataset.\n\n    Args:\n        split (str): The name of the split to load.\n        return_X_y (bool, optional): Whether to return X and y separately. Defaults to False.\n        sample_n_rows (int, optional): Number of rows to sample from the split. Defaults to None.\n        random_state (int, optional): Random state for sampling rows. Defaults to 36.\n\n    Returns:\n        Union[tuple[pd.DataFrame, np.array], pd.DataFrame]: The loaded split of the dataset.\n            If return_X_y is True, returns a tuple of X and y.\n            If return_X_y is False, returns a DataFrame with X and y as columns.\n    \"\"\"\n\n    if not self._is_data_splitted:\n        self._split_data()\n    if split not in self.splits.keys():\n        raise ValueError(\n            f\"Invalid Split: You requested split '{split}'. Valid splits are: {*list(self.splits.keys()),} \"\n        )\n    X, y = self[split][0], self[split][1]\n    if sample_n_rows is not None:\n        X = X.sample(sample_n_rows, random_state=random_state)\n        y = y[X.index]\n\n    if return_X_y:\n        return X, y\n    else:\n        return X.assign(**{self.target_column: y})\n</code></pre>"},{"location":"model_evaluator/","title":"Test","text":""},{"location":"model_evaluator/#model_forge.model.model_evaluator.ModelEvaluator","title":"<code>ModelEvaluator</code>","text":"<p>A class for evaluating machine learning models using cross-validation and multiple metrics.</p> <p>Attributes: - sklearn_metrics (List[str]): A list of sklearn metrics to be used for evaluation. - custom_scorers (dict, optional): A dictionary of custom scorers. Defaults to None. - cv (int, optional): The number of cross-validation folds. Defaults to 5.</p> Source code in <code>model_forge/model/model_evaluator.py</code> <pre><code>class ModelEvaluator:\n    \"\"\"\n    A class for evaluating machine learning models using cross-validation and multiple metrics.\n\n    Attributes:\n    - sklearn_metrics (List[str]): A list of sklearn metrics to be used for evaluation.\n    - custom_scorers (dict, optional): A dictionary of custom scorers. Defaults to None.\n    - cv (int, optional): The number of cross-validation folds. Defaults to 5.\n    \"\"\"\n\n    def __init__(self, metrics: dict[str, Callable], cv: int = 5) -&gt; None:\n        \"\"\"\n        Initialize the MetricEvaluator class.\n\n        Parameters:\n        - sklearn_metrics (List[str]): A list of sklearn metrics to be used for evaluation.\n        - metrics (dict, optional): A dictionary of metric scorers. Defaults to an empty dict.\n        - cv (int, optional): The number of cross-validation folds. Defaults to 5.\n        \"\"\"\n        self._metrics = metrics\n        self.cv = cv\n\n    @property\n    def metrics(self) -&gt; List[str]:\n        \"\"\"\n        Get a list of the evaluation metrics.\n\n        Returns:\n        - metric (Dict[str, Callable]): A mapping of evaluation metrics.\n        \"\"\"\n        return list(self._metrics.keys())\n\n    def evaluate(\n        self, model: CustomPipeline, X: np.array, y: np.array\n    ) -&gt; Dict[str, float]:\n        \"\"\"\n        Evaluate the model using cross-validation and multiple metrics.\n\n        Parameters:\n        - model: The machine learning model to be evaluated.\n        - X (array-like): The input features.\n        - y (array-like): The target variable.\n\n        Returns:\n            A dictionary containing the evaluation results for each metric.\n        \"\"\"\n        # TODO: re-write cross_validate to minimize dependence on sklearn\n        return cross_validate(\n            estimator=model, X=X, y=y, scoring=self._metrics, cv=self.cv\n        )\n</code></pre>"},{"location":"model_evaluator/#model_forge.model.model_evaluator.ModelEvaluator.metrics","title":"<code>metrics: List[str]</code>  <code>property</code>","text":"<p>Get a list of the evaluation metrics.</p> <p>Returns: - metric (Dict[str, Callable]): A mapping of evaluation metrics.</p>"},{"location":"model_evaluator/#model_forge.model.model_evaluator.ModelEvaluator.__init__","title":"<code>__init__(metrics, cv=5)</code>","text":"<p>Initialize the MetricEvaluator class.</p> <p>Parameters: - sklearn_metrics (List[str]): A list of sklearn metrics to be used for evaluation. - metrics (dict, optional): A dictionary of metric scorers. Defaults to an empty dict. - cv (int, optional): The number of cross-validation folds. Defaults to 5.</p> Source code in <code>model_forge/model/model_evaluator.py</code> <pre><code>def __init__(self, metrics: dict[str, Callable], cv: int = 5) -&gt; None:\n    \"\"\"\n    Initialize the MetricEvaluator class.\n\n    Parameters:\n    - sklearn_metrics (List[str]): A list of sklearn metrics to be used for evaluation.\n    - metrics (dict, optional): A dictionary of metric scorers. Defaults to an empty dict.\n    - cv (int, optional): The number of cross-validation folds. Defaults to 5.\n    \"\"\"\n    self._metrics = metrics\n    self.cv = cv\n</code></pre>"},{"location":"model_evaluator/#model_forge.model.model_evaluator.ModelEvaluator.evaluate","title":"<code>evaluate(model, X, y)</code>","text":"<p>Evaluate the model using cross-validation and multiple metrics.</p> <p>Parameters: - model: The machine learning model to be evaluated. - X (array-like): The input features. - y (array-like): The target variable.</p> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>A dictionary containing the evaluation results for each metric.</p> Source code in <code>model_forge/model/model_evaluator.py</code> <pre><code>def evaluate(\n    self, model: CustomPipeline, X: np.array, y: np.array\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Evaluate the model using cross-validation and multiple metrics.\n\n    Parameters:\n    - model: The machine learning model to be evaluated.\n    - X (array-like): The input features.\n    - y (array-like): The target variable.\n\n    Returns:\n        A dictionary containing the evaluation results for each metric.\n    \"\"\"\n    # TODO: re-write cross_validate to minimize dependence on sklearn\n    return cross_validate(\n        estimator=model, X=X, y=y, scoring=self._metrics, cv=self.cv\n    )\n</code></pre>"},{"location":"model_orchastrator/","title":"Model Orchastrator","text":"<p>Implemented a FactoryDesign pattern to create a pipeline based on the configuration provided. Implemented pipelines: 1. ModelPipeline 2. TuningPipeline</p>"},{"location":"model_orchastrator/#model_forge.model.model_orchastrator.CustomPipeline","title":"<code>CustomPipeline</code>","text":"<p>             Bases: <code>ABC</code>, <code>Pipeline</code></p> Source code in <code>model_forge/model/model_orchastrator.py</code> <pre><code>class CustomPipeline(ABC, Pipeline):\n    def transform_without_predictor(self, X):\n        \"\"\"\n        Transform the data without using the predictor step.\n\n        Args:\n            X (array-like): The input data to be transformed.\n\n        Returns:\n            array-like: The transformed data.\n        \"\"\"\n        # Add your code here to transform the data\n        return self[:-1].transform(X)\n\n    @property\n    def stepnames(self):\n        return list(self.named_steps.keys())\n\n    @staticmethod\n    def create_from_config():\n        ...\n</code></pre>"},{"location":"model_orchastrator/#model_forge.model.model_orchastrator.CustomPipeline.transform_without_predictor","title":"<code>transform_without_predictor(X)</code>","text":"<p>Transform the data without using the predictor step.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>The input data to be transformed.</p> required <p>Returns:</p> Type Description Source code in <code>model_forge/model/model_orchastrator.py</code> <pre><code>def transform_without_predictor(self, X):\n    \"\"\"\n    Transform the data without using the predictor step.\n\n    Args:\n        X (array-like): The input data to be transformed.\n\n    Returns:\n        array-like: The transformed data.\n    \"\"\"\n    # Add your code here to transform the data\n    return self[:-1].transform(X)\n</code></pre>"},{"location":"model_orchastrator/#model_forge.model.model_orchastrator.ModelOrchestrator","title":"<code>ModelOrchestrator</code>","text":"<p>             Bases: <code>Orchestartor</code></p> <p>Class representing the model orchestrator.</p> <p>This class is responsible for creating a model pipeline and managing the features.</p> <p>Attributes:</p> Name Type Description <code>cfg</code> <code>Config</code> <p>The configuration object.</p> <p>Methods:</p> Name Description <code>create_pipeline</code> <p>Creates a model pipeline based on the configuration.</p> <code>features_in</code> <p>Returns the features specified in the configuration.</p> Source code in <code>model_forge/model/model_orchastrator.py</code> <pre><code>class ModelOrchestrator(Orchestartor):\n    \"\"\"\n    Class representing the model orchestrator.\n\n    This class is responsible for creating a model pipeline and managing the features.\n\n    Attributes:\n        cfg (Config): The configuration object.\n\n    Methods:\n        create_pipeline: Creates a model pipeline based on the configuration.\n        features_in: Returns the features specified in the configuration.\n\n    \"\"\"\n\n    def create_pipeline(self):\n        \"\"\"\n        Creates a model pipeline based on the configuration.\n\n        Returns:\n            ModelPipeline: The created model pipeline.\n\n        \"\"\"\n        return ModelPipeline.create_from_config(self.cfg)\n\n    def features_in(self, cfg):\n        \"\"\"\n        Returns the features specified in the configuration.\n\n        Args:\n            cfg (Config): The configuration object.\n\n        Returns:\n            list: The list of features.\n\n        \"\"\"\n        return cfg.features\n</code></pre>"},{"location":"model_orchastrator/#model_forge.model.model_orchastrator.ModelOrchestrator.create_pipeline","title":"<code>create_pipeline()</code>","text":"<p>Creates a model pipeline based on the configuration.</p> <p>Returns:</p> Name Type Description <code>ModelPipeline</code> <p>The created model pipeline.</p> Source code in <code>model_forge/model/model_orchastrator.py</code> <pre><code>def create_pipeline(self):\n    \"\"\"\n    Creates a model pipeline based on the configuration.\n\n    Returns:\n        ModelPipeline: The created model pipeline.\n\n    \"\"\"\n    return ModelPipeline.create_from_config(self.cfg)\n</code></pre>"},{"location":"model_orchastrator/#model_forge.model.model_orchastrator.ModelOrchestrator.features_in","title":"<code>features_in(cfg)</code>","text":"<p>Returns the features specified in the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>Config</code> <p>The configuration object.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>The list of features.</p> Source code in <code>model_forge/model/model_orchastrator.py</code> <pre><code>def features_in(self, cfg):\n    \"\"\"\n    Returns the features specified in the configuration.\n\n    Args:\n        cfg (Config): The configuration object.\n\n    Returns:\n        list: The list of features.\n\n    \"\"\"\n    return cfg.features\n</code></pre>"},{"location":"model_orchastrator/#model_forge.model.model_orchastrator.ModelPipeline","title":"<code>ModelPipeline</code>","text":"<p>             Bases: <code>CustomPipeline</code></p> <p>Custom pipeline class for defining a model pipeline.</p> Source code in <code>model_forge/model/model_orchastrator.py</code> <pre><code>class ModelPipeline(CustomPipeline):\n    \"\"\"\n    Custom pipeline class for defining a model pipeline.\n    \"\"\"\n\n    @classmethod\n    def create_from_config(cls, cfg: DictConfig | OmegaConf) -&gt; \"ModelPipeline\":\n        \"\"\"\n        Create a custom model pipeline from the provided configuration.\n\n        Args:\n            cfg (DictConfig | OmegaConf): The configuration object.\n\n        Returns:\n            ModelPipeline: The created custom model pipeline.\n        \"\"\"\n        # First create list of tuples from the modelsteps list\n        pipeline_list = []\n        for i, step in enumerate(cfg.model.model_steps):\n            _step_dict = list(step.values())[0]\n            _step_name = list(step.keys())[0]\n            pipeline_list.append((_step_name, instantiate(_step_dict)))\n\n        # Create instance of cls\n        custom_pipeline = cls(steps=pipeline_list)\n        return custom_pipeline\n</code></pre>"},{"location":"model_orchastrator/#model_forge.model.model_orchastrator.ModelPipeline.create_from_config","title":"<code>create_from_config(cfg)</code>  <code>classmethod</code>","text":"<p>Create a custom model pipeline from the provided configuration.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig | OmegaConf</code> <p>The configuration object.</p> required <p>Returns:</p> Name Type Description <code>ModelPipeline</code> <code>ModelPipeline</code> <p>The created custom model pipeline.</p> Source code in <code>model_forge/model/model_orchastrator.py</code> <pre><code>@classmethod\ndef create_from_config(cls, cfg: DictConfig | OmegaConf) -&gt; \"ModelPipeline\":\n    \"\"\"\n    Create a custom model pipeline from the provided configuration.\n\n    Args:\n        cfg (DictConfig | OmegaConf): The configuration object.\n\n    Returns:\n        ModelPipeline: The created custom model pipeline.\n    \"\"\"\n    # First create list of tuples from the modelsteps list\n    pipeline_list = []\n    for i, step in enumerate(cfg.model.model_steps):\n        _step_dict = list(step.values())[0]\n        _step_name = list(step.keys())[0]\n        pipeline_list.append((_step_name, instantiate(_step_dict)))\n\n    # Create instance of cls\n    custom_pipeline = cls(steps=pipeline_list)\n    return custom_pipeline\n</code></pre>"},{"location":"model_orchastrator/#model_forge.model.model_orchastrator.Orchestartor","title":"<code>Orchestartor</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for orchestrating the model pipeline.</p> <p>Attributes:</p> Name Type Description <code>cfg</code> <code>dict</code> <p>Configuration parameters for the orchestrator.</p> <p>Methods:</p> Name Description <code>create_pipeline</code> <p>Abstract method for creating the model pipeline.</p> <code>features_in</code> <p>Abstract method for processing the input features.</p> Source code in <code>model_forge/model/model_orchastrator.py</code> <pre><code>class Orchestartor(ABC):\n    \"\"\"\n    Abstract base class for orchestrating the model pipeline.\n\n    Attributes:\n        cfg (dict): Configuration parameters for the orchestrator.\n\n    Methods:\n        create_pipeline(): Abstract method for creating the model pipeline.\n        features_in(cfg): Abstract method for processing the input features.\n\n    \"\"\"\n\n    def __init__(self, cfg) -&gt; None:\n        self.cfg = cfg\n\n    @abstractmethod\n    def create_pipeline(self):\n        ...\n\n    @abstractmethod\n    def features_in(self, cfg):\n        ...\n</code></pre>"},{"location":"model_orchastrator/#model_forge.model.model_orchastrator.TuningOrchestrator","title":"<code>TuningOrchestrator</code>","text":"<p>             Bases: <code>Orchestartor</code></p> <p>Class representing the orchestrator for model tuning.</p> Source code in <code>model_forge/model/model_orchastrator.py</code> <pre><code>class TuningOrchestrator(Orchestartor):\n\n    \"\"\"\n    Class representing the orchestrator for model tuning.\n    \"\"\"\n\n    def __init__(self, cfg, trial) -&gt; None:\n        super().__init__(cfg)\n        self.trial = trial\n\n    def create_pipeline(self):\n        \"\"\"\n        Creates a tuning pipeline based on the configuration and trial.\n        \"\"\"\n        return TuningPipeline.create_from_config(self.cfg, self.trial)\n\n    def features_in(self, cfg):\n        \"\"\"\n        Returns the features specified in the given configuration.\n        \"\"\"\n        return cfg.features\n</code></pre>"},{"location":"model_orchastrator/#model_forge.model.model_orchastrator.TuningOrchestrator.create_pipeline","title":"<code>create_pipeline()</code>","text":"<p>Creates a tuning pipeline based on the configuration and trial.</p> Source code in <code>model_forge/model/model_orchastrator.py</code> <pre><code>def create_pipeline(self):\n    \"\"\"\n    Creates a tuning pipeline based on the configuration and trial.\n    \"\"\"\n    return TuningPipeline.create_from_config(self.cfg, self.trial)\n</code></pre>"},{"location":"model_orchastrator/#model_forge.model.model_orchastrator.TuningOrchestrator.features_in","title":"<code>features_in(cfg)</code>","text":"<p>Returns the features specified in the given configuration.</p> Source code in <code>model_forge/model/model_orchastrator.py</code> <pre><code>def features_in(self, cfg):\n    \"\"\"\n    Returns the features specified in the given configuration.\n    \"\"\"\n    return cfg.features\n</code></pre>"},{"location":"model_orchastrator/#model_forge.model.model_orchastrator.TuningPipeline","title":"<code>TuningPipeline</code>","text":"<p>             Bases: <code>CustomPipeline</code></p> Source code in <code>model_forge/model/model_orchastrator.py</code> <pre><code>class TuningPipeline(CustomPipeline):\n    @classmethod\n    def create_from_config(cls, cfg: DictConfig | OmegaConf, trial) -&gt; \"TuningPipeline\":\n        \"\"\"\n        Create a custom model pipeline from the provided configuration.\n\n        Args:\n            cfg (DictConfig | OmegaConf): The configuration object.\n\n        Returns:\n            TuningPipeline: The created custom model pipeline.\n        \"\"\"\n        # First create list of tuples from the modelsteps list\n        params = cls.create_tuning_params(cfg, trial)\n        pipeline_list = []\n        for i, step in enumerate(cfg.model.model_steps):\n            _step_dict = next(iter(step.items()))\n\n            pipeline_list.append(\n                (str(i), instantiate(_step_dict[1], **(params[_step_dict[0]])))\n            )\n\n        # Create instance of cls\n        custom_pipeline = cls(steps=pipeline_list)\n        return custom_pipeline\n\n    @classmethod\n    def create_tuning_params(cls, cfg, trial):\n        \"\"\"\n        Create tuning parameters based on the provided configuration.\n\n        Args:\n            cfg (object): The configuration object containing hyperparameters.\n\n        Returns:\n            dict: A dictionary containing the tuning parameters for each step.\n\n        \"\"\"\n        params = {}\n        for step in cfg.hyperparameters:\n            params_steps = {}\n            for parameters in cfg.hyperparameters[step]:\n                parameter_trial = instantiate(parameters)\n                params_steps[\n                    parameter_trial.parameter_name\n                ] = parameter_trial.create_range(trial)\n            params[step] = params_steps\n        return params\n</code></pre>"},{"location":"model_orchastrator/#model_forge.model.model_orchastrator.TuningPipeline.create_from_config","title":"<code>create_from_config(cfg, trial)</code>  <code>classmethod</code>","text":"<p>Create a custom model pipeline from the provided configuration.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig | OmegaConf</code> <p>The configuration object.</p> required <p>Returns:</p> Name Type Description <code>TuningPipeline</code> <code>TuningPipeline</code> <p>The created custom model pipeline.</p> Source code in <code>model_forge/model/model_orchastrator.py</code> <pre><code>@classmethod\ndef create_from_config(cls, cfg: DictConfig | OmegaConf, trial) -&gt; \"TuningPipeline\":\n    \"\"\"\n    Create a custom model pipeline from the provided configuration.\n\n    Args:\n        cfg (DictConfig | OmegaConf): The configuration object.\n\n    Returns:\n        TuningPipeline: The created custom model pipeline.\n    \"\"\"\n    # First create list of tuples from the modelsteps list\n    params = cls.create_tuning_params(cfg, trial)\n    pipeline_list = []\n    for i, step in enumerate(cfg.model.model_steps):\n        _step_dict = next(iter(step.items()))\n\n        pipeline_list.append(\n            (str(i), instantiate(_step_dict[1], **(params[_step_dict[0]])))\n        )\n\n    # Create instance of cls\n    custom_pipeline = cls(steps=pipeline_list)\n    return custom_pipeline\n</code></pre>"},{"location":"model_orchastrator/#model_forge.model.model_orchastrator.TuningPipeline.create_tuning_params","title":"<code>create_tuning_params(cfg, trial)</code>  <code>classmethod</code>","text":"<p>Create tuning parameters based on the provided configuration.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>object</code> <p>The configuration object containing hyperparameters.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the tuning parameters for each step.</p> Source code in <code>model_forge/model/model_orchastrator.py</code> <pre><code>@classmethod\ndef create_tuning_params(cls, cfg, trial):\n    \"\"\"\n    Create tuning parameters based on the provided configuration.\n\n    Args:\n        cfg (object): The configuration object containing hyperparameters.\n\n    Returns:\n        dict: A dictionary containing the tuning parameters for each step.\n\n    \"\"\"\n    params = {}\n    for step in cfg.hyperparameters:\n        params_steps = {}\n        for parameters in cfg.hyperparameters[step]:\n            parameter_trial = instantiate(parameters)\n            params_steps[\n                parameter_trial.parameter_name\n            ] = parameter_trial.create_range(trial)\n        params[step] = params_steps\n    return params\n</code></pre>"},{"location":"predictions/","title":"Predictions","text":""},{"location":"predictions/#model_forge.data.predictions.Predictions","title":"<code>Predictions</code>","text":"<p>             Bases: <code>dict</code></p> <p>A dictionary-like class for storing predictions generated by a model on different data splits.</p> <p>Inherits from the built-in <code>dict</code> class.</p> <p>Methods: - create_from_model_dataset: Creates a <code>Predictions</code> object from a model and dataset.</p> <p>Attributes: - pred: A dictionary that stores the predictions for each data split.</p> Source code in <code>model_forge/data/predictions.py</code> <pre><code>class Predictions(dict):\n    \"\"\"\n    A dictionary-like class for storing predictions generated by a model on different data splits.\n\n    Inherits from the built-in `dict` class.\n\n    Methods:\n    - create_from_model_dataset: Creates a `Predictions` object from a model and dataset.\n\n    Attributes:\n    - pred: A dictionary that stores the predictions for each data split.\n\n    \"\"\"\n\n    @classmethod\n    def create_from_model_dataset(cls, model, dataset):\n        \"\"\"\n        Creates a `Predictions` object from a model and dataset.\n\n        Args:\n        - model: The model object used for making predictions.\n        - dataset: The dataset object containing the data splits.\n\n        Returns:\n        - A `Predictions` object containing the predictions for each data split.\n\n        Raises:\n        - AttributeError: If the model does not have a 'predict' attribute.\n\n        \"\"\"\n        pred = {}\n        for split, (X, _) in dataset:\n            if hasattr(model, \"predict\") and hasattr(model, \"predict_proba\"):\n                pred[split] = (model.predict(X), model.predict_proba(X))\n            elif hasattr(model, \"predict\"):\n                pred[split] = (model.predict(X), None)\n            else:\n                raise AttributeError(\"Model does not have 'predict' attribute.\")\n        return cls(pred)\n</code></pre>"},{"location":"predictions/#model_forge.data.predictions.Predictions.create_from_model_dataset","title":"<code>create_from_model_dataset(model, dataset)</code>  <code>classmethod</code>","text":"<p>Creates a <code>Predictions</code> object from a model and dataset.</p> <p>Args: - model: The model object used for making predictions. - dataset: The dataset object containing the data splits.</p> <p>Returns: - A <code>Predictions</code> object containing the predictions for each data split.</p> <p>Raises: - AttributeError: If the model does not have a 'predict' attribute.</p> Source code in <code>model_forge/data/predictions.py</code> <pre><code>@classmethod\ndef create_from_model_dataset(cls, model, dataset):\n    \"\"\"\n    Creates a `Predictions` object from a model and dataset.\n\n    Args:\n    - model: The model object used for making predictions.\n    - dataset: The dataset object containing the data splits.\n\n    Returns:\n    - A `Predictions` object containing the predictions for each data split.\n\n    Raises:\n    - AttributeError: If the model does not have a 'predict' attribute.\n\n    \"\"\"\n    pred = {}\n    for split, (X, _) in dataset:\n        if hasattr(model, \"predict\") and hasattr(model, \"predict_proba\"):\n            pred[split] = (model.predict(X), model.predict_proba(X))\n        elif hasattr(model, \"predict\"):\n            pred[split] = (model.predict(X), None)\n        else:\n            raise AttributeError(\"Model does not have 'predict' attribute.\")\n    return cls(pred)\n</code></pre>"}]}